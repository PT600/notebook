# KL Divergence vs Variation of Information
   - KL Divergence gives us a distance between _two distributions_ over the **same variable** or set of variables. In contrast, Variation of Information gives us distance between two **jointly distributed** variables.
   - KL Divergence is between distributions, Variation of Information within a distribution(jointly)!

# Referrer:
   * [Visual Information] (http://colah.github.io/posts/2015-09-Visual-Information/)
