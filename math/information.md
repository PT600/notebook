# KL Divergence vs Variation of Information
    * KL Divergence gives us a distance between *two distributions* over the *same variable* or set of variables. In contrast, Variation of Information gives us distance between two *jointly distributed* variables.
    * KL Divergence is between distributions, Variation of Information within a distribution(jointly)!

#Referrer:
   * [Visual Information] (http://colah.github.io/posts/2015-09-Visual-Information/)
